{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>speech</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>I embrace with great satisfaction the opportun...</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington</td>\n",
       "      <td>In resuming your consultations for the general...</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Among the many interesting objects which will ...</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Washington</td>\n",
       "      <td>A free people ought not only to be armed, but ...</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    president                                             speech  year\n",
       "0  Washington  Fellow-Citizens of the Senate and House of Rep...  1790\n",
       "1  Washington  I embrace with great satisfaction the opportun...  1790\n",
       "2  Washington  In resuming your consultations for the general...  1790\n",
       "3  Washington  Among the many interesting objects which will ...  1790\n",
       "4  Washington  A free people ought not only to be armed, but ...  1790"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data set\n",
    "data = pd.read_table(\"speech_data_extend.txt\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-z]')\n",
    "docs_list = [doc.lower() for doc in list(data.loc[:,'speech'])]\n",
    "clean_docs = [nltk.word_tokenize(regex.sub(' ', ''.join(doc))) for doc in docs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'of', 'the', 'senate', 'and', 'house', 'of', 'representatives'], ['i', 'embrace', 'with', 'great', 'satisfaction', 'the', 'opportunity', 'which', 'now', 'presents', 'itself', 'of', 'congratulating', 'you', 'on', 'the', 'present', 'favorable', 'prospects', 'of', 'our', 'public', 'affairs', 'the', 'recent', 'accession', 'of', 'the', 'important', 'state', 'of', 'north', 'carolina', 'to', 'the', 'constitution', 'of', 'the', 'united', 'states', 'of', 'which', 'official', 'information', 'has', 'been', 'received', 'the', 'rising', 'credit', 'and', 'respectability', 'of', 'our', 'country', 'the', 'general', 'and', 'increasing', 'good', 'will', 'toward', 'the', 'government', 'of', 'the', 'union', 'and', 'the', 'concord', 'peace', 'and', 'plenty', 'with', 'which', 'we', 'are', 'blessed', 'are', 'circumstances', 'auspicious', 'in', 'an', 'eminent', 'degree', 'to', 'our', 'national', 'prosperity'], ['in', 'resuming', 'your', 'consultations', 'for', 'the', 'general', 'good', 'you', 'can', 'not', 'but', 'derive', 'encouragement', 'from', 'the', 'reflection', 'that', 'the', 'measures', 'of', 'the', 'last', 'session', 'have', 'been', 'as', 'satisfactory', 'to', 'your', 'constituents', 'as', 'the', 'novelty', 'and', 'difficulty', 'of', 'the', 'work', 'allowed', 'you', 'to', 'hope', 'still', 'further', 'to', 'realize', 'their', 'expectations', 'and', 'to', 'secure', 'the', 'blessings', 'which', 'a', 'gracious', 'providence', 'has', 'placed', 'within', 'our', 'reach', 'will', 'in', 'the', 'course', 'of', 'the', 'present', 'important', 'session', 'call', 'for', 'the', 'cool', 'and', 'deliberate', 'exertion', 'of', 'your', 'patriotism', 'firmness', 'and', 'wisdom'], ['among', 'the', 'many', 'interesting', 'objects', 'which', 'will', 'engage', 'your', 'attention', 'that', 'of', 'providing', 'for', 'the', 'common', 'defense', 'will', 'merit', 'particular', 'regard', 'to', 'be', 'prepared', 'for', 'war', 'is', 'one', 'of', 'the', 'most', 'effectual', 'means', 'of', 'preserving', 'peace'], ['a', 'free', 'people', 'ought', 'not', 'only', 'to', 'be', 'armed', 'but', 'disciplined', 'to', 'which', 'end', 'a', 'uniform', 'and', 'well', 'digested', 'plan', 'is', 'requisite', 'and', 'their', 'safety', 'and', 'interest', 'require', 'that', 'they', 'should', 'promote', 'such', 'manufactories', 'as', 'tend', 'to', 'render', 'them', 'independent', 'of', 'others', 'for', 'essential', 'particularly', 'military', 'supplies']]\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing and non-alphabetic character removal done\n",
    "print(clean_docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of', 'the', 'Senate', 'and', 'House']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabetic_tokens = []\n",
    "for i in tokens:\n",
    "    if i.isalpha():\n",
    "        alphabetic_tokens.append(i)\n",
    "alphabetic_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizen', 'senat', 'hous', 'repres'], ['embrac', 'great', 'satisfact', 'opportun', 'present', 'congratul', 'present', 'favor', 'prospect', 'public', 'affair', 'recent', 'access', 'import', 'state', 'north', 'carolina', 'constitut', 'unit', 'state', 'offici', 'inform', 'receiv', 'rise', 'credit', 'respect', 'countri', 'gener', 'increas', 'good', 'toward', 'govern', 'union', 'concord', 'peac', 'plenti', 'bless', 'circumst', 'auspici', 'emin', 'degre', 'nation', 'prosper'], ['resum', 'consult', 'gener', 'good', 'deriv', 'encourag', 'reflect', 'measur', 'last', 'session', 'satisfactori', 'constitu', 'novelti', 'difficulti', 'work', 'allow', 'hope', 'still', 'realiz', 'expect', 'secur', 'bless', 'graciou', 'provid', 'place', 'within', 'reach', 'cours', 'present', 'import', 'session', 'call', 'cool', 'deliber', 'exert', 'patriot', 'firm', 'wisdom'], ['among', 'mani', 'interest', 'object', 'engag', 'attent', 'provid', 'common', 'defens', 'merit', 'particular', 'regard', 'prepar', 'war', 'one', 'effectu', 'mean', 'preserv', 'peac'], ['free', 'peopl', 'ought', 'arm', 'disciplin', 'end', 'uniform', 'well', 'digest', 'plan', 'requisit', 'safeti', 'interest', 'requir', 'promot', 'manufactori', 'tend', 'render', 'independ', 'other', 'essenti', 'particularli', 'militari', 'suppli']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "clean_docs_stops = []\n",
    "for doc in clean_docs:\n",
    "    doc_wo_stops = [stemmer.stem(word) for word in doc if not word in stop_words]\n",
    "    clean_docs_stops.append(doc_wo_stops)\n",
    "print(clean_docs_stops[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(clean_docs)\n",
    "\n",
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all unique stems/terms\n",
    "unique = set(word for doc in clean_docs_stops for word in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "A = [[1,2,3,4],[5,6],[7,8],[9,10]]\n",
    "B = [item for sublist in A for item in sublist]\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount_by_doc = [dict.fromkeys(list(unique),0) for i in range(len(clean_docs_stops))]\n",
    "for index,doc in enumerate(clean_docs_stops):\n",
    "    for word in doc:\n",
    "        if word in unique:\n",
    "            wordcount_by_doc[index][word] = wordcount_by_doc[index][word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fellow 1\n",
      "hous 1\n",
      "senat 1\n",
      "repres 1\n",
      "citizen 1\n"
     ]
    }
   ],
   "source": [
    "for word,count in wordcount_by_doc[0].items():\n",
    "    if count!=0 :\n",
    "        print(word,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_matrix = pd.DataFrame(wordcount_by_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "13550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dt_matrix.head()\n",
    "print(test.shape[0])\n",
    "print(test.shape[1])\n",
    "tfdv_matrix = np.zeros(shape=(test.shape[0], test.shape[1]))\n",
    "tfdv_matrix\n",
    "test.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get corpus level tf-idf scores of each term\n",
    "\n",
    "# calculate tf_v\n",
    "\n",
    "tf_v = 1 + np.log(dt_matrix.sum(axis = 0))\n",
    "\n",
    "# calculate df_v - find number of non-zero rows for each column\n",
    "\n",
    "df_v = dt_matrix.astype(bool).sum(axis=0)\n",
    "\n",
    "idf_v = np.log(dt_matrix.shape[0]/df_v)\n",
    "\n",
    "# calculate tf-idf\n",
    "\n",
    "tf_idf_v = tf_v * idf_v\n",
    "\n",
    "# calculate tf_dv\n",
    "#tfdv_matrix = np.zeros(shape=(dt_matrix.shape[0], dt_matrix.shape[1]))\n",
    "#for i in range(dt_matrix.shape[0]):\n",
    "#    for j,value in enumerate(dt_matrix.iloc[i,:]):\n",
    "#        tfdv_matrix[i][j] = (1 + np.log(dt_matrix.iloc[i,j]) if dt_matrix.iloc[i,j]!=0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum(test.iloc[:,0] != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wool            35.215236\n",
       "cabl            35.124009\n",
       "circuit         34.474063\n",
       "acr             34.403554\n",
       "texa            34.232973\n",
       "solar           34.065523\n",
       "pound           34.001592\n",
       "seal            33.954782\n",
       "ton             33.729550\n",
       "forest          33.660847\n",
       "iraqi           33.608646\n",
       "ounc            33.555389\n",
       "inch            33.543376\n",
       "battleship      33.387505\n",
       "injunct         33.364720\n",
       "domingo         33.361819\n",
       "filipino        33.275431\n",
       "gun             33.250876\n",
       "isthmu          33.240694\n",
       "silver          33.220076\n",
       "refuge          33.219595\n",
       "hondura         33.201740\n",
       "timber          33.193219\n",
       "sugar           33.168913\n",
       "iraq            33.140268\n",
       "park            33.138446\n",
       "leas            33.028476\n",
       "santo           33.012980\n",
       "immigr          32.946556\n",
       "space           32.866600\n",
       "                  ...    \n",
       "litter          10.049966\n",
       "littleton       10.049966\n",
       "zoom            10.049966\n",
       "liveri          10.049966\n",
       "lizzi           10.049966\n",
       "loaf            10.049966\n",
       "liliuokolani    10.049966\n",
       "lighterag       10.049966\n",
       "legate          10.049966\n",
       "lieberman       10.049966\n",
       "legitimaci      10.049966\n",
       "legum           10.049966\n",
       "lemon           10.049966\n",
       "lenienc         10.049966\n",
       "leningrad       10.049966\n",
       "leniti          10.049966\n",
       "lenni           10.049966\n",
       "leper           10.049966\n",
       "leprosi         10.049966\n",
       "lessli          10.049966\n",
       "lessor          10.049966\n",
       "letterman       10.049966\n",
       "letup           10.049966\n",
       "lew             10.049966\n",
       "liaison         10.049966\n",
       "lianna          10.049966\n",
       "libyaninspir    10.049966\n",
       "license         10.049966\n",
       "lick            10.049966\n",
       "aa              10.049966\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_v.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the idf - column vector\n",
    "\n",
    "#tfdv_matrix = np.zeros(shape=(dt_matrix.shape[0], dt_matrix.shape[1]))\n",
    "\n",
    "#for i in range(tfdv_matrix.shape[0]):\n",
    "#    tfidf_matrix[i] = tfdv_matrix[i] * idfv\n",
    "\n",
    "# sum of tfidf score over documents - column sum\n",
    "\n",
    "#tfidf_matrix.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append metadata to document term matrix\n",
    "dt_matrix['meta_president'] = data['president']\n",
    "dt_matrix['meta_year'] = data['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aana</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abat</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>abdic</th>\n",
       "      <th>abduct</th>\n",
       "      <th>...</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zollverein</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolog</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuloaga</th>\n",
       "      <th>meta_president</th>\n",
       "      <th>meta_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13552 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aana  aaron  abandon  abat  abba  abbrevi  abdic  abduct  \\\n",
       "0   0    0     0      0        0     0     0        0      0       0   \n",
       "1   0    0     0      0        0     0     0        0      0       0   \n",
       "2   0    0     0      0        0     0     0        0      0       0   \n",
       "3   0    0     0      0        0     0     0        0      0       0   \n",
       "4   0    0     0      0        0     0     0        0      0       0   \n",
       "\n",
       "     ...      zimbabwean  zinc  zion  zollverein  zone  zoolog  zoom  zuloaga  \\\n",
       "0    ...               0     0     0           0     0       0     0        0   \n",
       "1    ...               0     0     0           0     0       0     0        0   \n",
       "2    ...               0     0     0           0     0       0     0        0   \n",
       "3    ...               0     0     0           0     0       0     0        0   \n",
       "4    ...               0     0     0           0     0       0     0        0   \n",
       "\n",
       "   meta_president  meta_year  \n",
       "0      Washington       1790  \n",
       "1      Washington       1790  \n",
       "2      Washington       1790  \n",
       "3      Washington       1790  \n",
       "4      Washington       1790  \n",
       "\n",
       "[5 rows x 13552 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dt_matrix and tf-idf to avoid computation later\n",
    "\n",
    "dt_matrix.to_pickle('dtmatrix.pkl')\n",
    "tf_idf_v.to_pickle('tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_matrix = pd.read_pickle('dtmatrix.pkl')\n",
    "tf_idf_v = pd.read_pickle('tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
