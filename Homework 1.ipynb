{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data set\n",
    "data = pd.read_table(\"speech_data_extend.txt\")\n",
    "data.head()\n",
    "regex = re.compile('[^a-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_list = [doc.lower() for doc in list(data.speech)]\n",
    "clean_docs = [nltk.word_tokenize(regex.sub(' ', ''.join(doc))) for doc in docs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'of', 'the', 'senate', 'and', 'house', 'of', 'representatives'], ['i', 'embrace', 'with', 'great', 'satisfaction', 'the', 'opportunity', 'which', 'now', 'presents', 'itself', 'of', 'congratulating', 'you', 'on', 'the', 'present', 'favorable', 'prospects', 'of', 'our', 'public', 'affairs', 'the', 'recent', 'accession', 'of', 'the', 'important', 'state', 'of', 'north', 'carolina', 'to', 'the', 'constitution', 'of', 'the', 'united', 'states', 'of', 'which', 'official', 'information', 'has', 'been', 'received', 'the', 'rising', 'credit', 'and', 'respectability', 'of', 'our', 'country', 'the', 'general', 'and', 'increasing', 'good', 'will', 'toward', 'the', 'government', 'of', 'the', 'union', 'and', 'the', 'concord', 'peace', 'and', 'plenty', 'with', 'which', 'we', 'are', 'blessed', 'are', 'circumstances', 'auspicious', 'in', 'an', 'eminent', 'degree', 'to', 'our', 'national', 'prosperity'], ['in', 'resuming', 'your', 'consultations', 'for', 'the', 'general', 'good', 'you', 'can', 'not', 'but', 'derive', 'encouragement', 'from', 'the', 'reflection', 'that', 'the', 'measures', 'of', 'the', 'last', 'session', 'have', 'been', 'as', 'satisfactory', 'to', 'your', 'constituents', 'as', 'the', 'novelty', 'and', 'difficulty', 'of', 'the', 'work', 'allowed', 'you', 'to', 'hope', 'still', 'further', 'to', 'realize', 'their', 'expectations', 'and', 'to', 'secure', 'the', 'blessings', 'which', 'a', 'gracious', 'providence', 'has', 'placed', 'within', 'our', 'reach', 'will', 'in', 'the', 'course', 'of', 'the', 'present', 'important', 'session', 'call', 'for', 'the', 'cool', 'and', 'deliberate', 'exertion', 'of', 'your', 'patriotism', 'firmness', 'and', 'wisdom'], ['among', 'the', 'many', 'interesting', 'objects', 'which', 'will', 'engage', 'your', 'attention', 'that', 'of', 'providing', 'for', 'the', 'common', 'defense', 'will', 'merit', 'particular', 'regard', 'to', 'be', 'prepared', 'for', 'war', 'is', 'one', 'of', 'the', 'most', 'effectual', 'means', 'of', 'preserving', 'peace'], ['a', 'free', 'people', 'ought', 'not', 'only', 'to', 'be', 'armed', 'but', 'disciplined', 'to', 'which', 'end', 'a', 'uniform', 'and', 'well', 'digested', 'plan', 'is', 'requisite', 'and', 'their', 'safety', 'and', 'interest', 'require', 'that', 'they', 'should', 'promote', 'such', 'manufactories', 'as', 'tend', 'to', 'render', 'them', 'independent', 'of', 'others', 'for', 'essential', 'particularly', 'military', 'supplies']]\n"
     ]
    }
   ],
   "source": [
    "print(clean_docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of', 'the', 'Senate', 'and', 'House']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabetic_tokens = []\n",
    "for i in tokens:\n",
    "    if i.isalpha():\n",
    "        alphabetic_tokens.append(i)\n",
    "alphabetic_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizen', 'senat', 'hous', 'repres'], ['embrac', 'great', 'satisfact', 'opportun', 'present', 'congratul', 'present', 'favor', 'prospect', 'public', 'affair', 'recent', 'access', 'import', 'state', 'north', 'carolina', 'constitut', 'unit', 'state', 'offici', 'inform', 'receiv', 'rise', 'credit', 'respect', 'countri', 'gener', 'increas', 'good', 'toward', 'govern', 'union', 'concord', 'peac', 'plenti', 'bless', 'circumst', 'auspici', 'emin', 'degre', 'nation', 'prosper'], ['resum', 'consult', 'gener', 'good', 'deriv', 'encourag', 'reflect', 'measur', 'last', 'session', 'satisfactori', 'constitu', 'novelti', 'difficulti', 'work', 'allow', 'hope', 'still', 'realiz', 'expect', 'secur', 'bless', 'graciou', 'provid', 'place', 'within', 'reach', 'cours', 'present', 'import', 'session', 'call', 'cool', 'deliber', 'exert', 'patriot', 'firm', 'wisdom'], ['among', 'mani', 'interest', 'object', 'engag', 'attent', 'provid', 'common', 'defens', 'merit', 'particular', 'regard', 'prepar', 'war', 'one', 'effectu', 'mean', 'preserv', 'peac'], ['free', 'peopl', 'ought', 'arm', 'disciplin', 'end', 'uniform', 'well', 'digest', 'plan', 'requisit', 'safeti', 'interest', 'requir', 'promot', 'manufactori', 'tend', 'render', 'independ', 'other', 'essenti', 'particularli', 'militari', 'suppli']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "clean_docs_stops = []\n",
    "for doc in clean_docs:\n",
    "    doc_wo_stops = [stemmer.stem(word) for word in doc if not word in stop_words]\n",
    "    clean_docs_stops.append(doc_wo_stops)\n",
    "print(clean_docs_stops[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(clean_docs)\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique = set(item for sublist in clean_docs_stops for item in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "A = [[1,2,3,4],[5,6],[7,8],[9,10]]\n",
    "B = [item for sublist in A for item in sublist]\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bye': 1, 'hello': 1, 'yo': 0},\n",
       " {'bye': 3, 'hello': 0, 'yo': 0},\n",
       " {'bye': 1, 'hello': 1, 'yo': 1}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount_by_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount_by_doc = [dict.fromkeys(list(unique),0) for i in range(len(clean_docs_stops))]\n",
    "for index,doc in enumerate(clean_docs_stops):\n",
    "    for word in doc:\n",
    "        if word in unique:\n",
    "            wordcount_by_doc[index][word] = wordcount_by_doc[index][word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fellow 1\n",
      "hous 1\n",
      "senat 1\n",
      "citizen 1\n",
      "repres 1\n"
     ]
    }
   ],
   "source": [
    "for word,count in wordcount_by_doc[0].items():\n",
    "    if count!=0 :\n",
    "        print(word,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_matrix = pd.DataFrame(wordcount_by_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "13550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dt_matrix.head()\n",
    "print(test.shape[0])\n",
    "print(test.shape[1])\n",
    "tfdv_matrix = np.zeros(shape=(test.shape[0], test.shape[1]))\n",
    "tfdv_matrix\n",
    "test.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate tf_dv\n",
    "tfdv_matrix = np.zeros(shape=(dt_matrix.shape[0], dt_matrix.shape[1]))\n",
    "for i in range(dt_matrix.shape[0]):\n",
    "    for j,value in enumerate(dt_matrix.iloc[i,:]):\n",
    "        tfdv_matrix[i][j] = (1 + np.log(dt_matrix.iloc[i,j]) if dt_matrix.iloc[i,j]!=0 else 0)\n",
    "\n",
    "# calculate df_v - find number of non-zero rows for each column\n",
    "\n",
    "dfv_matrix = np.zeros(shape=(1, dt_matrix.shape[1]))\n",
    "for i in range(dt_matrix.shape[1]):\n",
    "    sum(dt_matrix.iloc[:,i] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test.iloc[:,0] != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the idf - column vector\n",
    "\n",
    "tfdv_matrix = np.zeros(shape=(dt_matrix.shape[0], dt_matrix.shape[1]))\n",
    "\n",
    "idfv = np.log(dt_matrix.shape[0]/dfv_matrix)\n",
    "for i in range(tfdv_matrix.shape[0]):\n",
    "    tfidf_matrix[i] = tfdv_matrix[i] * idfv\n",
    "\n",
    "# sum of tfidf score over documents - column sum\n",
    "\n",
    "tfidf_matrix.sum(axis = 0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
